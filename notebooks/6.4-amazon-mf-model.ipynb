{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bdf6dc-3ed2-4e70-a6ef-080c0e5c0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/fs1/miniconda3/envs/gml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66721fd-ba3f-4665-a595-a91cfedf0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a623f3f-64b5-4843-a579-5cecfdf5ba18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9328b0590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4591bc95-71a6-4472-8a6c-2ff4c0798522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Any, model_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves model in gzip format\n",
    "\n",
    "    Args:\n",
    "        model: Model to be saved\n",
    "        model_path: Path to save model to\n",
    "        \n",
    "    Returns:\n",
    "        (None)\n",
    "    \"\"\"\n",
    "    with gzip.open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f'Model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee658bc-dc25-4697-90d5-3057ff4b92fd",
   "metadata": {},
   "source": [
    "## Create the MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a81f20-efd4-4fbc-ba13-d9dc55ace4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_l2(array):\n",
    "    loss = torch.sum(array ** 2)\n",
    "    return loss\n",
    "\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dim, c_vector=1e-6):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size # size of the dictionary of embeddings\n",
    "        self.emb_dim = emb_dim # size of each embedding vector\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(emb_size, emb_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        # loss\n",
    "        self.bce = nn.BCELoss()\n",
    "        \n",
    "        print(f'Model initialized: {self}')\n",
    "        \n",
    "    def forward(self, product1, product2):\n",
    "        emb_product1 = self.embedding(product1)\n",
    "        emb_product2 = self.embedding(product2)\n",
    "        interaction = self.sig(torch.sum(emb_product1*emb_product2, dim = 1, dtype = torch.float))\n",
    "        return interaction\n",
    "    \n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        mf_loss = self.bce(pred, label)\n",
    "        \n",
    "        # L2 regularization\n",
    "        product_prior = refularize_l2(self.embedding.weight) * self.c_vector\n",
    "        \n",
    "        loss_total  = mf_loss + product_prior # loss + regularization \n",
    "        \n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee543547-3620-4d69-8095-f79c8ab0a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: MF(\n",
      "  (embedding): Embedding(1000, 12)\n",
      "  (sig): Sigmoid()\n",
      "  (bce): BCELoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MF(\n",
       "  (embedding): Embedding(1000, 12)\n",
       "  (sig): Sigmoid()\n",
       "  (bce): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF(1000, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02dedf-e5f2-459f-b72b-8ace2e121bd2",
   "metadata": {},
   "source": [
    "## Create the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63746617-ea27-42c8-b6a8-146c12600746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc427ddb-a8d8-4555-995d-f9e9ed58b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences:\n",
    "    NEGATIVE_SAMPLE_TABLE_SIZE = 1e7\n",
    "    WINDOW = 5\n",
    "    \n",
    "    def __init__(self, sequence_path: str, val_path: str, subsample: float = 0.001, power: float = 0.75):\n",
    "        \"\"\"\n",
    "        Intialize the dataset object\n",
    "        \"\"\"\n",
    "        self.negative_idx = 0\n",
    "        self.n_unique_tokens = 0\n",
    "        \n",
    "        self.sequences = np.load(sequence_path).tolist()\n",
    "        self.n_sequences = len(self.sequences)\n",
    "        print(f'# Sequences = {self.n_sequences}')\n",
    "        \n",
    "        self.val = pd.read_csv(val_path)\n",
    "        print(f'# Validation data = {self.val.shape}')\n",
    "        \n",
    "        self.word_freq = self.get_word_freq()\n",
    "        \n",
    "        self.word2id, self.id2word = self.get_mapping_dicts()\n",
    "        self.add_val_product_to_mapping_dict()\n",
    "        self.n_unique_tokens = len(self.word2id)\n",
    "        print(f'# Tokens = {self.n_unique_tokens}')\n",
    "        \n",
    "        sequence_file_name = Path(sequence_path).resolve().stem\n",
    "        save_model(self.word2id, f'../data/processed/{sequence_file_name}_word2id')\n",
    "        save_model(self.id2word, f'../data/processed/{sequence_file_name}_id2word')\n",
    "        \n",
    "        self.sequences = self.convert_sequences_to_id()\n",
    "        self.word_freq = self.convert_word_freq_to_id()\n",
    "        \n",
    "        self.discard_probs = self.get_discard_probs(sample = subsample)\n",
    "        \n",
    "        self.neg_table = self.get_negative_sample_table(power = power)\n",
    "        \n",
    "    def get_word_freq(self) -> Counter:\n",
    "        \"\"\"\n",
    "        Returns a dictionary of word frequencies\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_flat = list(itertools.chain.from_iterable(self.sequences)) # flatten the array\n",
    "        \n",
    "        word_freq = Counter(seq_flat)\n",
    "        \n",
    "        return word_freq\n",
    "    \n",
    "    def get_mapping_dicts(self):\n",
    "        word2id = dict()\n",
    "        id2word = dict()\n",
    "        \n",
    "        wid = 0\n",
    "        for w,c in self.word_freq.items():\n",
    "            word2id[w] = wid\n",
    "            id2word[wid] = w\n",
    "            wid += 1\n",
    "        \n",
    "        return word2id, id2word\n",
    "    \n",
    "    def add_val_product_to_mapping_dicts(self):\n",
    "        val_product_set = set(self.val['product1'].values).union(set(self.val['product2'].values))\n",
    "        \n",
    "        print(f'Size of word2id before adding val product : {len(self.word2id)}')\n",
    "        wid = max(self.word2id.values()) + 1\n",
    "        for w in val_product_set:\n",
    "            if w in self.word2id:\n",
    "                self.word2id[w] = wid\n",
    "                self.id2word[wid] = w\n",
    "                wid +=1\n",
    "        \n",
    "        self.val = None # free up space\n",
    "        print(f'Size of the word2id after adding vcal product : {len(self.word2id)}')\n",
    "        \n",
    "                \n",
    "    def convert_sequece_to_id(self):\n",
    "        return np.vectorize(self.word2id.get)(self.sequences)\n",
    "    \n",
    "    def get_product_id(self, x):\n",
    "        return self.word2id.get(x, -1)\n",
    "    \n",
    "    def convert_word_freq_to_id(self):\n",
    "        return {self.word2id[k] : v for k ,v  in self.word_freq.items()}\n",
    "    \n",
    "    def get_discard_probs(self, sample = 0.001):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of words and their associated discard probability, \n",
    "        word should ne discarded if np.random.rand() < probability\n",
    "        \"\"\"\n",
    "        \n",
    "        # convert to array\n",
    "        word_freq = np.array(list(self.word_freq.items()), dtype=np.float64)\n",
    "        \n",
    "        # convert to probability\n",
    "        word_freq[:, 1] = word_freq[:, 1] / word_freq[:, 1].sum()\n",
    "        \n",
    "        # perform subsampling \n",
    "        # http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "        word_freq[:, 1] = (np.sqrt(word_freq[:, 1]/ sample) + 1) * (sample / word_freq[:, 1]) \n",
    "        \n",
    "        # get dict \n",
    "        discard_probs = {int(k) : v for k, v in word_freq.tolist()}\n",
    "        \n",
    "        return discard_probs\n",
    "    \n",
    "    def get_negative_sample_table(self, power=0.75):\n",
    "        \"\"\"\n",
    "        Returns a table with size = NEGATIVE_SAMPLE_TABLE_SIZE of nagative samples which can be selected via indexing. \n",
    "        \"\"\"\n",
    "        \n",
    "        # COnvert to array \n",
    "        word_freq = np.array(list(self.word_freq.items()), dtype = np.float)\n",
    "        \n",
    "        # adjust the power\n",
    "        word_freq[:, 1] = word_freq[:, 1] ** power\n",
    "        \n",
    "        # Get probabilities\n",
    "        word_freq_sum = word_freq[:, 1] ** power\n",
    "        word_freq[:, 1] = word_freq[:, 1] / word_freq_sum\n",
    "        \n",
    "        # Multiply probabilities by sample table size\n",
    "        word_freq[:, 1] = np.round(word_freq[:. 1] * self.NEGATIVE_SAMPLE_TABLE_SIZE)\n",
    "        \n",
    "        # Convert to int \n",
    "        word_freq = word_freq.astype(int).tolist()\n",
    "        \n",
    "        # create the sample table\n",
    "        sample_table = [[tup[0]*tup[1]] for tup in word_freq]\n",
    "        sample_table = np.array(list(itertools.chain.from_iterable(sample_table)))\n",
    "        np.random.shuffle(sample_table)\n",
    "\n",
    "        return sample_table\n",
    "        \n",
    "    \n",
    "    def get_pairs(self, idx, window = 5):\n",
    "        pairs = []\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        for center_idx, node in enumerate(sequence):\n",
    "            for i in range(-window, window + 1):\n",
    "                context_idx = center_idx + i\n",
    "                if (context_idx > 0) and (context_idx < len(sequence)) and (node != sequence[context_idx]) and (np.random.rand() < self.discard_probs[sequence[context_idx]]):\n",
    "                    pairs.append((node, sequence[context_idx]))\n",
    "    \n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def get_all_center_context_pair(self, window = 5) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Returns a list of tuples (center, context).\n",
    "        \n",
    "        Args: \n",
    "            window:\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        pairs = []\n",
    "        \n",
    "        for sequence in self.sequences:\n",
    "            for center_idx, node in enumerate(sequence):\n",
    "                context_idx = center_idx + i\n",
    "                    if (0 <= context_idx < len(sequence)) \\\n",
    "                        and node != sequence[context_idx] \\\n",
    "                        and np.random.rand() < self.discard_probs[sequence[context_idx]]:\n",
    "                        pairs.append((node, sequence[context_idx]))\n",
    "                        \n",
    "            \n",
    "        return pairs\n",
    "    \n",
    "    \n",
    "    def get_negative_samples(self, context, sample_size = 5) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns a list of negative samples, where len = sample_size.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "            sample_size:\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            neg_sample = self.neg_table[self.negative_idx:self.negative_idx + sample_size]\n",
    "            \n",
    "            self.negative_idx = (self.negative_idx + sample_size) % len(self.neg_table)\n",
    "            \n",
    "            if len(neg_sample) != sample_size:\n",
    "                neg_sample = np.concatenate((neg_sample, \n",
    "                                             self.neg_table[:self.negative_idx]))\n",
    "                \n",
    "            \n",
    "            if not context in neg_sample:\n",
    "                return neg_sample\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gml]",
   "language": "python",
   "name": "conda-env-gml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
