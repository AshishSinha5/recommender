{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3e61860630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "torch.cuda.is_available() = True\n",
      "torch.cuda.device_count() = 7\n",
      "torch.cuda.get_device_name(1) = 'NVIDIA GeForce RTX 2080 Ti'\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # Get PyTorch and CUDA version\n",
    "print(f\"{torch.cuda.is_available() = }\") # Check that CUDA works\n",
    "print(f\"{torch.cuda.device_count() = }\") # Check how many CUDA capable devices you have\n",
    "# Print device human readable names\n",
    "print(f\"{torch.cuda.get_device_name(1) = }\")\n",
    "# Add more lines with +1 like get_device_name(3), get_device_name(4) if you have more devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Any, model_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves model in gzip format\n",
    "\n",
    "    Args:\n",
    "        model: Model to be saved\n",
    "        model_path: Path to save model to\n",
    "        \n",
    "    Returns:\n",
    "        (None)\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    with gzip.open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f'Model saved to {model_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_sizes: dict, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.emb_sizes = emb_sizes\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Create the embedding layer\n",
    "        self.center_embeddings = nn.ModuleList()\n",
    "        for k, v in self.emb_sizes.items():\n",
    "            self.center_embeddings.append(nn.Embedding(v, emb_dim, sparse=True))\n",
    "        \n",
    "        self.context_embeddings = nn.ModuleList()\n",
    "        for k, v in self.emb_sizes.items():\n",
    "            self.context_embeddings.append(nn.Embedding(v, emb_dim, sparse=True))\n",
    "        \n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        \"\"\"\n",
    "        Init embeddings like word2vec\n",
    "\n",
    "        Center embeddings have uniform disribution ~ [-0.5/emb_dim, 0.5/emb_dim]\n",
    "        Context embeddings have nitialized with 0\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Initializing embeddings:\n",
    "        # https://stackoverflow.com/questions/55276504/different-methods-for-initializing-embedding-layer-weights-in-pytorch\n",
    "        for emb in self.context_embeddings:\n",
    "            emb.weight.data.uniform_(0, 0)\n",
    "\n",
    "        for emb in self.center_embeddings:\n",
    "            emb.weight.data.uniform_(-0.5/self.emb_dim, 0.5/self.emb_dim)\n",
    "\n",
    "    def forward(self, centers, contexts, neg_contexts):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            center: List of center words\n",
    "            contexts: List of context words\n",
    "            neg_contexts: List of list of negative sample words\n",
    "\n",
    "        Return\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate positive score\n",
    "        emb_centers  = []\n",
    "        for i in range(centers.shape[0]):\n",
    "            emb_centers.append(self.center_embeddings[i](centers[:, i]))\n",
    "        emb_center = torch.mean(torch.stack(emb_centers), axis = 1)\n",
    "\n",
    "        emb_contexts = []\n",
    "        for i in  range(contexts.shape[0]):\n",
    "            emb_contexts.append(self.context_embeddings[i](contexts[:, i]))\n",
    "        emb_context = torch.mean(torch.stack(emb_contexts), axis = 1)\n",
    "\n",
    "        emb_neg_contexts = []\n",
    "        neg_contexts = neg_contexts.view(-1, len(self.context_embeddings))\n",
    "        for i in range(neg_contexts.shape[1]):\n",
    "            emb_neg_contexts.append(self.context_embeddings[i](neg_contexts[:, i]))\n",
    "        emb_neg_context = torch.mean(torch.stack(emb_neg_contexts), axis = 1)\n",
    "\n",
    "\n",
    "        score = torch.mul(emb_center, emb_context) \n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = torch.clamp(score, max = 10, min = -10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "\n",
    "        # Negative Score\n",
    "        neg_score = torch.bmm(emb_neg_context.view(emb_center.shape[0], -1, emb_center.shape[1]), emb_center.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, mix=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)\n",
    "    \n",
    "    def get_center_emb(self, centers):\n",
    "        emb_centers = []\n",
    "        for row_idx, center in enumerate(centers):\n",
    "            emb_center = []\n",
    "            for col_idx, center_ in enumerate(center):\n",
    "                emb_center.append(self.center_embeddings[col_idx](center_))\n",
    "            emb_center.append(torch.mean(torch.stack(emb_center), axis = 1))\n",
    "\n",
    "        \n",
    "        return torch.stack(emb_centers)\n",
    "    \n",
    "\n",
    "    def save_embeddings(self, file_name):\n",
    "        embeddings = self.center_embeddings.weight.cpu().data.numpy()\n",
    "        np.save(file_name, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram({'a': 10}, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (center_embeddings): ModuleList(\n",
       "    (0): Embedding(10, 100, sparse=True)\n",
       "  )\n",
       "  (context_embeddings): ModuleList(\n",
       "    (0): Embedding(10, 100, sparse=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences:\n",
    "\n",
    "    def __init__(self, sequence_path: str, val_path: str, meta_path: str, subsample: float = 0.001, \n",
    "                 power: float = 0.75):\n",
    "        \n",
    "        self.negative_idx = 0\n",
    "        self.n_unique_tokens = 0\n",
    "        self.META_COLS = ['category_level_1', 'category_level_2', 'brand', 'price']\n",
    "\n",
    "        self.sequences = np.load(sequence_path).tolist()\n",
    "        self.n_sequences = len(self.sequences)\n",
    "        print(f'No. of sequences = {self.n_sequences}')\n",
    "\n",
    "        self.val = pd.read_csv(val_path)\n",
    "        print(f'Shape of validation data = {self.val.shape}')\n",
    "\n",
    "        self.word_freq = self.get_word_freq()\n",
    "\n",
    "        self.word2id, self.id2word = self.get_mapping_dicts()\n",
    "        self.add_val_product_to_mapping_dict()\n",
    "        self.n_unique_tokens = len(self.word2id)\n",
    "        print(f'No. of unique tokens = {self.n_unique_tokens}')\n",
    "\n",
    "    def get_word_freq(self):\n",
    "        seq_flat = list(itertools.chain.from_iterable(self.sequences))\n",
    "        word_freq = Counter(seq_flat)\n",
    "\n",
    "        return word_freq\n",
    "\n",
    "    \n",
    "    def get_mapping_dicts(self):\n",
    "        word2id = dict()\n",
    "        id2word = dict()\n",
    "\n",
    "        wid = 0\n",
    "\n",
    "        for w, c in self.word_freq:\n",
    "            word2id[w] = wid\n",
    "            id2word[wid] = w\n",
    "            wid += 1\n",
    "\n",
    "        return word2id, id2word\n",
    "\n",
    "\n",
    "    def add_val_product_to_mapping_dict(self):\n",
    "        val_product_set = set(self.val['product1'].values).union(set(self.val['product2'].values))\n",
    "        print(f'Original size of the mapping dict = {len(self.word2id)}')\n",
    "\n",
    "        wid = max(self.word2id.values) + 1\n",
    "        for w in val_product_set:\n",
    "            if w not in self.word2id:\n",
    "                self.word2id[w] = wid\n",
    "                self.id2word[wid] = w\n",
    "                wid += 1\n",
    "\n",
    "        self.val = None\n",
    "        print(f'Size of mapping dict after adding val products = {len(self.word2id)}')\n",
    "\n",
    "    def convert_seq_to_ids(self):\n",
    "        return np.vectorize(self.word2id.get)(self.sequences)\n",
    "    \n",
    "    def convert_word_freq_to_ids(self):\n",
    "        return {self.word2id[w]:c for w,c in self.word_freq.items}\n",
    "    \n",
    "    def get_product_id(self, x):\n",
    "        return self.word2id.get(x, -1)\n",
    "\n",
    "    def prep_meta(self):\n",
    "        pass\n",
    "\n",
    "    def convert_meta_to_dict(self):\n",
    "        pass\n",
    "\n",
    "    def get_discard_prob(self):\n",
    "        pass\n",
    "\n",
    "    def get_negative_sample_table(self):\n",
    "        pass\n",
    "\n",
    "    def get_mata(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
