## TODO list

- ~~Do eda of the single chunk of data
    - ~~What are the most popular subreddits?~~
    - ~~Which users are most active?~
    - ~~How many users are there?~
    - ~~How many subreddits are there?
    - ~~What is the distribution of unique commentors per subreddit?
    - ~~What is the distribution of number of comments a user has made to different subreddit?

- Do cleaning based on findings of first eda
    - ~~Remove ['deleted'] user from data
    - ~~Remove subreddits with less than 10 comments
    
- Create a directory Structure for keeping cleaned trasformed and raw data - 
    - external       <- Data from third party sources.
    - ~~raw            <- The original, immutable data dump.
    - processed      <- The final, canonical data sets for modeling.
    - interim        <- Intermediate data that has been transformed.
    - ~~cleaned        <- cleaned data after removing data points
    
- Do EDA again
    - ~~How much data is lost if we remove the subreddits that have very less comments
- ~~Based on the above EDA decide weather the data is sufficient for the similarity based recommender system without any cleaning ~~

- Come up with a plan to trim the userbase and subreddits based on the EDA

- Write an procedure to evaluate a recommender model
    - Train test split stategy 
- Create a script to find the data summary
- Create script to decode data
   
- cookiecut from the repo - https://github.com/drivendata/cookiecutter-data-science 


    

